{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PR Manipulation Experiment - Analysis\n",
    "\n",
    "Analysis and visualization of LLM-generated press releases across different stakes conditions.\n",
    "\n",
    "## Research Question\n",
    "\n",
    "Do higher stakes (legal exposure, harm severity, public attention) increase fact omission and fabrication in LLM-generated corporate communications?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load results\ndf = pd.read_csv('data/results.csv')\n\n# Display basic info\nprint(f\"Total runs: {len(df)}\")\nprint(f\"Scenarios: {df['scenario'].nunique()}\")\nprint(f\"Stakes tiers: {df['stakes_tier'].nunique()}\")\nprint(f\"Models: {df['model'].nunique()}\")\nprint(f\"\\nFirst few rows:\")\ndf.head()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall statistics\n",
    "print(\"Overall Disclosure and Fabrication Statistics:\")\n",
    "print(df[['disclosure_score', 'fabrication_count']].describe())\n",
    "\n",
    "# By stakes tier\n",
    "print(\"\\nBy Stakes Tier:\")\n",
    "stakes_summary = df.groupby('stakes_tier')[['disclosure_score', 'fabrication_count']].agg(['mean', 'std', 'min', 'max'])\n",
    "print(stakes_summary)\n",
    "\n",
    "# By model\n",
    "print(\"\\nBy Model:\")\n",
    "model_summary = df.groupby('model')[['disclosure_score', 'fabrication_count']].agg(['mean', 'std', 'min', 'max'])\n",
    "print(model_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 1: Heatmap - Disclosure Score by Model × Stakes Tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean disclosure score for each model × stakes tier combination\n",
    "heatmap_data = df.pivot_table(\n",
    "    values='disclosure_score',\n",
    "    index='model',\n",
    "    columns='stakes_tier',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "# Reorder columns to: low, medium, high\n",
    "heatmap_data = heatmap_data[['low', 'medium', 'high']]\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(\n",
    "    heatmap_data,\n",
    "    annot=True,\n",
    "    fmt='.3f',\n",
    "    cmap='RdYlGn',\n",
    "    vmin=0,\n",
    "    vmax=1,\n",
    "    cbar_kws={'label': 'Mean Disclosure Score'},\n",
    "    linewidths=1,\n",
    "    linecolor='gray'\n",
    ")\n",
    "plt.title('Mean Disclosure Score by Model and Stakes Tier', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Stakes Tier', fontsize=12)\n",
    "plt.ylabel('Model', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('heatmap_disclosure.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 2: Bar Chart - Mean Fabrication Count by Stakes Tier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean and std for fabrication count by stakes tier\n",
    "fab_by_stakes = df.groupby('stakes_tier')['fabrication_count'].agg(['mean', 'std']).reset_index()\n",
    "fab_by_stakes = fab_by_stakes.set_index('stakes_tier').reindex(['low', 'medium', 'high']).reset_index()\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(\n",
    "    fab_by_stakes['stakes_tier'],\n",
    "    fab_by_stakes['mean'],\n",
    "    yerr=fab_by_stakes['std'],\n",
    "    capsize=5,\n",
    "    color=['#2ecc71', '#f39c12', '#e74c3c'],\n",
    "    alpha=0.8,\n",
    "    edgecolor='black',\n",
    "    linewidth=1.5\n",
    ")\n",
    "\n",
    "plt.title('Mean Fabrication Count by Stakes Tier', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Stakes Tier', fontsize=12)\n",
    "plt.ylabel('Mean Fabrication Count', fontsize=12)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, bar in enumerate(bars):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.2f}',\n",
    "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('bar_fabrication.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 3: Line Plot - Disclosure Score Across Stakes Tiers (per Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean disclosure score for each model × stakes tier\n",
    "line_data = df.groupby(['model', 'stakes_tier'])['disclosure_score'].mean().reset_index()\n",
    "\n",
    "# Define stakes tier order\n",
    "stakes_order = ['low', 'medium', 'high']\n",
    "line_data['stakes_tier'] = pd.Categorical(line_data['stakes_tier'], categories=stakes_order, ordered=True)\n",
    "line_data = line_data.sort_values('stakes_tier')\n",
    "\n",
    "# Create line plot\n",
    "plt.figure(figsize=(12, 7))\n",
    "\n",
    "for model in df['model'].unique():\n",
    "    model_data = line_data[line_data['model'] == model]\n",
    "    plt.plot(\n",
    "        model_data['stakes_tier'],\n",
    "        model_data['disclosure_score'],\n",
    "        marker='o',\n",
    "        markersize=8,\n",
    "        linewidth=2.5,\n",
    "        label=model\n",
    "    )\n",
    "\n",
    "plt.title('Disclosure Score Across Stakes Tiers by Model', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Stakes Tier', fontsize=12)\n",
    "plt.ylabel('Mean Disclosure Score', fontsize=12)\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(alpha=0.3)\n",
    "plt.legend(title='Model', fontsize=11, title_fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('line_disclosure.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization 4: Box Plot - Distribution of Scores by Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots for disclosure and fabrication\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Box plot for disclosure score\n",
    "sns.boxplot(\n",
    "    data=df,\n",
    "    x='scenario_name',\n",
    "    y='disclosure_score',\n",
    "    ax=axes[0],\n",
    "    palette='Set2'\n",
    ")\n",
    "axes[0].set_title('Disclosure Score Distribution by Scenario', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlabel('Scenario', fontsize=11)\n",
    "axes[0].set_ylabel('Disclosure Score', fontsize=11)\n",
    "axes[0].set_ylim(0, 1)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Box plot for fabrication count\n",
    "sns.boxplot(\n",
    "    data=df,\n",
    "    x='scenario_name',\n",
    "    y='fabrication_count',\n",
    "    ax=axes[1],\n",
    "    palette='Set2'\n",
    ")\n",
    "axes[1].set_title('Fabrication Count Distribution by Scenario', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlabel('Scenario', fontsize=11)\n",
    "axes[1].set_ylabel('Fabrication Count', fontsize=11)\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('boxplot_scenarios.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Fact Disclosure Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate disclosure rate for each fact\n",
    "fact_columns = ['F1_disclosed', 'F2_disclosed', 'F3_disclosed', 'F4_disclosed']\n",
    "\n",
    "# Overall disclosure rates\n",
    "print(\"Overall Fact Disclosure Rates:\")\n",
    "for col in fact_columns:\n",
    "    rate = df[col].mean()\n",
    "    print(f\"{col}: {rate:.2%}\")\n",
    "\n",
    "# By stakes tier\n",
    "print(\"\\nFact Disclosure Rates by Stakes Tier:\")\n",
    "fact_by_stakes = df.groupby('stakes_tier')[fact_columns].mean()\n",
    "fact_by_stakes = fact_by_stakes.reindex(['low', 'medium', 'high'])\n",
    "print(fact_by_stakes)\n",
    "\n",
    "# Visualize fact disclosure by stakes tier\n",
    "plt.figure(figsize=(12, 6))\n",
    "fact_by_stakes.T.plot(kind='bar', ax=plt.gca())\n",
    "plt.title('Individual Fact Disclosure Rates by Stakes Tier', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Fact', fontsize=12)\n",
    "plt.ylabel('Disclosure Rate', fontsize=12)\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(title='Stakes Tier', fontsize=11)\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('facts_disclosure.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Test if disclosure score differs significantly across stakes tiers\n",
    "low_disclosure = df[df['stakes_tier'] == 'low']['disclosure_score']\n",
    "medium_disclosure = df[df['stakes_tier'] == 'medium']['disclosure_score']\n",
    "high_disclosure = df[df['stakes_tier'] == 'high']['disclosure_score']\n",
    "\n",
    "# ANOVA\n",
    "f_stat, p_value = stats.f_oneway(low_disclosure, medium_disclosure, high_disclosure)\n",
    "print(f\"ANOVA - Disclosure Score by Stakes Tier:\")\n",
    "print(f\"F-statistic: {f_stat:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "print(f\"Significant at α=0.05: {p_value < 0.05}\\n\")\n",
    "\n",
    "# Test if fabrication count differs across stakes tiers\n",
    "low_fab = df[df['stakes_tier'] == 'low']['fabrication_count']\n",
    "medium_fab = df[df['stakes_tier'] == 'medium']['fabrication_count']\n",
    "high_fab = df[df['stakes_tier'] == 'high']['fabrication_count']\n",
    "\n",
    "# Kruskal-Wallis (non-parametric alternative to ANOVA)\n",
    "h_stat, p_value_kw = stats.kruskal(low_fab, medium_fab, high_fab)\n",
    "print(f\"Kruskal-Wallis - Fabrication Count by Stakes Tier:\")\n",
    "print(f\"H-statistic: {h_stat:.4f}\")\n",
    "print(f\"P-value: {p_value_kw:.4f}\")\n",
    "print(f\"Significant at α=0.05: {p_value_kw < 0.05}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Finding 1: Disclosure by stakes\n",
    "mean_disclosure_by_stakes = df.groupby('stakes_tier')['disclosure_score'].mean().reindex(['low', 'medium', 'high'])\n",
    "print(\"\\n1. DISCLOSURE SCORE BY STAKES TIER:\")\n",
    "for tier, score in mean_disclosure_by_stakes.items():\n",
    "    print(f\"   {tier.capitalize()}: {score:.3f}\")\n",
    "\n",
    "# Finding 2: Fabrication by stakes\n",
    "mean_fab_by_stakes = df.groupby('stakes_tier')['fabrication_count'].mean().reindex(['low', 'medium', 'high'])\n",
    "print(\"\\n2. FABRICATION COUNT BY STAKES TIER:\")\n",
    "for tier, count in mean_fab_by_stakes.items():\n",
    "    print(f\"   {tier.capitalize()}: {count:.2f}\")\n",
    "\n",
    "# Finding 3: Model comparison\n",
    "mean_disclosure_by_model = df.groupby('model')['disclosure_score'].mean().sort_values(ascending=False)\n",
    "print(\"\\n3. DISCLOSURE SCORE BY MODEL (most honest to least):\")\n",
    "for model, score in mean_disclosure_by_model.items():\n",
    "    print(f\"   {model}: {score:.3f}\")\n",
    "\n",
    "# Finding 4: Scenario comparison\n",
    "mean_disclosure_by_scenario = df.groupby('scenario_name')['disclosure_score'].mean().sort_values(ascending=False)\n",
    "print(\"\\n4. DISCLOSURE SCORE BY SCENARIO (most honest to least):\")\n",
    "for scenario, score in mean_disclosure_by_scenario.items():\n",
    "    print(f\"   {scenario}: {score:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}