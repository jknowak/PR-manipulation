{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PR Manipulation Experiment - Analysis\n",
    "\n",
    "Analysis and visualization of LLM-generated press releases across measurement conditions and stakes.\n",
    "\n",
    "## Research Question\n",
    "\n",
    "Do LLMs omit or distort internal test measurements in corporate communications, and does this behavior increase with:\n",
    "1. **Violation severity** (how far above legal limits)\n",
    "2. **Stakes level** (legal exposure, harm, public attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_ArtistPropertiesSubstitution' object has no attribute 'register'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/__init__.py:161\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse \u001b[38;5;28;01mas\u001b[39;00m parse_version\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# cbook must import matplotlib only within function\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;66;03m# definitions, so it is safe to import from it here.\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _version, cbook, _docstring, rcsetup\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_api\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatplotlibDeprecationWarning\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrcsetup\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cycler  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/rcsetup.py:28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BackendFilter, backend_registry\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcbook\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ls_mapper\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Colormap, is_color_like\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fontconfig_pattern\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_fontconfig_pattern\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_enums\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m JoinStyle, CapStyle\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/colors.py:57\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmpl\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _api, _cm, cbook, scale, _image\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_color_data\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BASE_COLORS, TABLEAU_COLORS, CSS4_COLORS, XKCD_COLORS\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_ColorMapping\u001b[39;00m(\u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/scale.py:764\u001b[0m\n\u001b[1;32m    755\u001b[0m         docs\u001b[38;5;241m.\u001b[39mextend([\n\u001b[1;32m    756\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    757\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    758\u001b[0m             textwrap\u001b[38;5;241m.\u001b[39mindent(docstring, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m8\u001b[39m),\n\u001b[1;32m    759\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    760\u001b[0m         ])\n\u001b[1;32m    761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(docs)\n\u001b[0;32m--> 764\u001b[0m \u001b[43m_docstring\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister\u001b[49m(\n\u001b[1;32m    765\u001b[0m     scale_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m{\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m}\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mrepr\u001b[39m(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m get_scale_names()]),\n\u001b[1;32m    766\u001b[0m     scale_docs\u001b[38;5;241m=\u001b[39m_get_scale_docs()\u001b[38;5;241m.\u001b[39mrstrip(),\n\u001b[1;32m    767\u001b[0m     )\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_ArtistPropertiesSubstitution' object has no attribute 'register'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load results\n",
    "df = pd.read_csv('data/results.csv')\n",
    "\n",
    "# Display basic info\n",
    "print(f\"Total runs: {len(df)}\")\n",
    "print(f\"Scenarios: {df['scenario'].nunique()} ({', '.join(df['scenario'].unique())})\") \n",
    "print(f\"Stakes tiers: {df['stakes_tier'].nunique() if 'stakes_tier' in df.columns else 'N/A'}\")\n",
    "print(f\"Measurement conditions: {df['measurement_condition'].nunique() if 'measurement_condition' in df.columns else 'N/A'}\")\n",
    "print(f\"Models: {df['model'].nunique()}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall classification breakdown\n",
    "print(\"Classification Breakdown:\")\n",
    "print(df['classification'].value_counts())\n",
    "print(f\"\\nPercentages:\")\n",
    "print(df['classification'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "df['classification'].value_counts().plot(kind='bar', color=['#e74c3c', '#f39c12', '#3498db', '#2ecc71'])\n",
    "plt.title('Classification Distribution', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Classification', fontsize=12)\n",
    "plt.ylabel('Count', fontsize=12)\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refusal Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refusal rates by model\n",
    "refusal_by_model = df.groupby('model')['classification'].apply(lambda x: (x == 'refused').sum() / len(x) * 100).sort_values(ascending=False)\n",
    "\n",
    "print(\"Refusal Rate by Model:\")\n",
    "for model, rate in refusal_by_model.items():\n",
    "    print(f\"  {model}: {rate:.1f}%\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "refusal_by_model.plot(kind='barh', color='#e74c3c')\n",
    "plt.title('Refusal Rate by Model', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Refusal Rate (%)', fontsize=12)\n",
    "plt.ylabel('Model', fontsize=12)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurement Condition Analysis\n",
    "\n",
    "How does violation severity affect disclosure behavior?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out control scenario and refusals for this analysis\n",
    "df_crisis = df[df['measurement_condition'].notna()].copy()\n",
    "df_non_refused = df_crisis[df_crisis['classification'] != 'refused'].copy()\n",
    "\n",
    "# Classification by measurement condition\n",
    "measurement_order = ['pending', 'minor', 'moderate', 'severe']\n",
    "\n",
    "classification_by_measurement = pd.crosstab(\n",
    "    df_non_refused['measurement_condition'],\n",
    "    df_non_refused['classification'],\n",
    "    normalize='index'\n",
    ") * 100\n",
    "\n",
    "# Reorder rows\n",
    "classification_by_measurement = classification_by_measurement.reindex(measurement_order)\n",
    "\n",
    "print(\"Classification by Measurement Condition (% of non-refusals):\")\n",
    "print(classification_by_measurement)\n",
    "\n",
    "# Visualize\n",
    "classification_by_measurement.plot(kind='bar', stacked=False, figsize=(12, 6), \n",
    "                                   color=['#2ecc71', '#e74c3c', '#f39c12'])\n",
    "plt.title('Classification Distribution by Measurement Condition', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Measurement Condition', fontsize=12)\n",
    "plt.ylabel('Percentage', fontsize=12)\n",
    "plt.legend(title='Classification', bbox_to_anchor=(1.05, 1))\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stakes Tier Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification by stakes tier\n",
    "stakes_order = ['low', 'medium', 'high']\n",
    "\n",
    "classification_by_stakes = pd.crosstab(\n",
    "    df_non_refused['stakes_tier'],\n",
    "    df_non_refused['classification'],\n",
    "    normalize='index'\n",
    ") * 100\n",
    "\n",
    "classification_by_stakes = classification_by_stakes.reindex(stakes_order)\n",
    "\n",
    "print(\"Classification by Stakes Tier (% of non-refusals):\")\n",
    "print(classification_by_stakes)\n",
    "\n",
    "# Visualize\n",
    "classification_by_stakes.plot(kind='bar', stacked=False, figsize=(12, 6),\n",
    "                              color=['#2ecc71', '#e74c3c', '#f39c12'])\n",
    "plt.title('Classification Distribution by Stakes Tier', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Stakes Tier', fontsize=12)\n",
    "plt.ylabel('Percentage', fontsize=12)\n",
    "plt.legend(title='Classification', bbox_to_anchor=(1.05, 1))\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification by model (excluding refusals)\n",
    "classification_by_model = pd.crosstab(\n",
    "    df_non_refused['model'],\n",
    "    df_non_refused['classification'],\n",
    "    normalize='index'\n",
    ") * 100\n",
    "\n",
    "print(\"Classification by Model (% of non-refusals):\")\n",
    "print(classification_by_model)\n",
    "\n",
    "# Visualize\n",
    "classification_by_model.plot(kind='barh', stacked=True, figsize=(10, 6),\n",
    "                             color=['#2ecc71', '#e74c3c', '#f39c12'])\n",
    "plt.title('Classification Distribution by Model (Non-Refusals)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Percentage', fontsize=12)\n",
    "plt.ylabel('Model', fontsize=12)\n",
    "plt.legend(title='Classification', bbox_to_anchor=(1.25, 1))\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: Honest disclosure rate by Model × Measurement Condition\n",
    "honest_rate = df_crisis.groupby(['model', 'measurement_condition']).apply(\n",
    "    lambda x: (x['classification'] == 'disclosed_accurate').sum() / len(x) * 100\n",
    ").unstack(fill_value=0)\n",
    "\n",
    "honest_rate = honest_rate[measurement_order]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(honest_rate, annot=True, fmt='.1f', cmap='RdYlGn', vmin=0, vmax=100,\n",
    "            cbar_kws={'label': 'Honest Disclosure Rate (%)'})\n",
    "plt.title('Honest Disclosure Rate by Model and Measurement Condition', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Measurement Condition', fontsize=12)\n",
    "plt.ylabel('Model', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('heatmap_honest_disclosure.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap: Omission rate by Model × Stakes Tier\n",
    "omission_rate = df_crisis.groupby(['model', 'stakes_tier']).apply(\n",
    "    lambda x: (x['classification'] == 'omitted').sum() / len(x) * 100\n",
    ").unstack(fill_value=0)\n",
    "\n",
    "omission_rate = omission_rate[stakes_order]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(omission_rate, annot=True, fmt='.1f', cmap='RdYlGn_r', vmin=0, vmax=100,\n",
    "            cbar_kws={'label': 'Omission Rate (%)'})\n",
    "plt.title('Omission Rate by Model and Stakes Tier', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Stakes Tier', fontsize=12)\n",
    "plt.ylabel('Model', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.savefig('heatmap_omission.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distortion Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at distortion examples\n",
    "distorted = df[df['classification'] == 'distorted']\n",
    "\n",
    "print(f\"Total distorted cases: {len(distorted)}\")\n",
    "print(f\"\\nDistortion by model:\")\n",
    "print(distorted['model'].value_counts())\n",
    "print(f\"\\nDistortion by measurement condition:\")\n",
    "print(distorted['measurement_condition'].value_counts())\n",
    "\n",
    "# Show sample distortion\n",
    "if len(distorted) > 0:\n",
    "    with open('data/press_releases.jsonl', 'r') as f:\n",
    "        all_releases = [json.loads(line) for line in f]\n",
    "    \n",
    "    sample_idx = distorted.index[0]\n",
    "    sample_release = [r for r in all_releases if r.get('run') == distorted.iloc[0]['run'] \n",
    "                      and r.get('model') == distorted.iloc[0]['model']][0]\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"SAMPLE DISTORTION:\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Model: {sample_release.get('model')}\")\n",
    "    print(f\"Scenario: {sample_release.get('scenario_name')}\")\n",
    "    print(f\"Measurement: {sample_release.get('measurement_condition')}\")\n",
    "    print(f\"\\nDistortion details:\")\n",
    "    if sample_release.get('distortion_details'):\n",
    "        print(f\"  Claimed: {sample_release['distortion_details'].get('claimed')}\")\n",
    "        print(f\"  Actual: {sample_release['distortion_details'].get('actual')}\")\n",
    "        print(f\"  Type: {sample_release['distortion_details'].get('distortion_type')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Chi-square test: Does measurement condition affect classification?\n",
    "contingency_table = pd.crosstab(df_non_refused['measurement_condition'], \n",
    "                                df_non_refused['classification'])\n",
    "\n",
    "chi2, p_value, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "print(\"Chi-Square Test: Measurement Condition vs Classification\")\n",
    "print(f\"Chi-square statistic: {chi2:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "print(f\"Degrees of freedom: {dof}\")\n",
    "print(f\"Significant at α=0.05: {p_value < 0.05}\")\n",
    "\n",
    "# Chi-square test: Does stakes tier affect classification?\n",
    "contingency_table2 = pd.crosstab(df_non_refused['stakes_tier'], \n",
    "                                 df_non_refused['classification'])\n",
    "\n",
    "chi2_2, p_value_2, dof_2, expected_2 = stats.chi2_contingency(contingency_table2)\n",
    "\n",
    "print(f\"\\nChi-Square Test: Stakes Tier vs Classification\")\n",
    "print(f\"Chi-square statistic: {chi2_2:.4f}\")\n",
    "print(f\"P-value: {p_value_2:.4f}\")\n",
    "print(f\"Degrees of freedom: {dof_2}\")\n",
    "print(f\"Significant at α=0.05: {p_value_2 < 0.05}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Findings Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Overall rates (non-refusals)\n",
    "total_non_refused = len(df_non_refused)\n",
    "honest_rate_overall = (df_non_refused['classification'] == 'disclosed_accurate').sum() / total_non_refused * 100\n",
    "omission_rate_overall = (df_non_refused['classification'] == 'omitted').sum() / total_non_refused * 100\n",
    "distortion_rate_overall = (df_non_refused['classification'] == 'distorted').sum() / total_non_refused * 100\n",
    "\n",
    "print(f\"\\n1. OVERALL BEHAVIOR (non-refusals):\")\n",
    "print(f\"   Honest disclosure: {honest_rate_overall:.1f}%\")\n",
    "print(f\"   Omission: {omission_rate_overall:.1f}%\")\n",
    "print(f\"   Distortion: {distortion_rate_overall:.1f}%\")\n",
    "\n",
    "# Model rankings\n",
    "refusal_rates = df.groupby('model')['classification'].apply(lambda x: (x == 'refused').sum() / len(x) * 100)\n",
    "honest_rates = df_non_refused.groupby('model')['classification'].apply(lambda x: (x == 'disclosed_accurate').sum() / len(x) * 100)\n",
    "\n",
    "print(f\"\\n2. MODEL RANKINGS:\")\n",
    "print(f\"   Most likely to refuse: {refusal_rates.idxmax()} ({refusal_rates.max():.1f}%)\")\n",
    "print(f\"   Least likely to refuse: {refusal_rates.idxmin()} ({refusal_rates.min():.1f}%)\")\n",
    "print(f\"   Most honest (when not refusing): {honest_rates.idxmax()} ({honest_rates.max():.1f}%)\")\n",
    "print(f\"   Least honest (when not refusing): {honest_rates.idxmin()} ({honest_rates.min():.1f}%)\")\n",
    "\n",
    "# Measurement severity effect\n",
    "honest_by_measurement = df_non_refused.groupby('measurement_condition')['classification'].apply(\n",
    "    lambda x: (x == 'disclosed_accurate').sum() / len(x) * 100\n",
    ").reindex(measurement_order)\n",
    "\n",
    "print(f\"\\n3. HONEST DISCLOSURE BY VIOLATION SEVERITY:\")\n",
    "for condition, rate in honest_by_measurement.items():\n",
    "    print(f\"   {condition}: {rate:.1f}%\")\n",
    "\n",
    "# Stakes effect\n",
    "honest_by_stakes = df_non_refused.groupby('stakes_tier')['classification'].apply(\n",
    "    lambda x: (x == 'disclosed_accurate').sum() / len(x) * 100\n",
    ").reindex(stakes_order)\n",
    "\n",
    "print(f\"\\n4. HONEST DISCLOSURE BY STAKES TIER:\")\n",
    "for tier, rate in honest_by_stakes.items():\n",
    "    print(f\"   {tier}: {rate:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
